---
layout: default
title: "BASICS OF MACHINE LEARNING"
date: 2025-09-25
categories: [machine-learning]
---

# Basics of Machine Learning

## 1. Supervised Learning

**Definition:**  
The algorithm learns from a labeled dataset, where input data has known output.

**Goal:**  
To learn the relationship between input and output for prediction on unseen data.

**Example:**  
Predicting student placement (Yes/No) based on IQ and CGPA. “Placement” is the output label.

**Sub-Categories:**
- **Regression:** Predicts numerical values (e.g., house price, salary).  
- **Classification:** Predicts categorical values (e.g., Yes/No, Spam/Not Spam).

## 2. Unsupervised Learning

**Definition:**  
The algorithm works with unlabeled data — only inputs are given.

**Goal:**  
To find hidden patterns or structure in the data.

**Sub-Categories & Applications:**
- **Clustering:** Groups similar data (e.g., customer segmentation).  
- **Dimensionality Reduction:** Reduces feature count for simplification or visualization (e.g., PCA).  
- **Anomaly Detection:** Finds unusual data points (e.g., fraud detection).  
- **Association Rule Learning:** Discovers variable relationships (e.g., “diapers → beer” rule).

## 3. Semi-Supervised Learning

**Definition:**  
Uses a small labeled dataset with a large unlabeled one.

**Goal:**  
Improves learning efficiency when labeling is costly or limited.

**Example:**  
Google Photos uses one labeled image to identify similar unlabeled photos.

## 4. Reinforcement Learning

**Definition:**  
An agent learns by interacting with an environment using trial and error.

**Goal:**  
To learn a strategy (policy) that maximizes cumulative reward.

**Process:**  
The agent performs actions, gets feedback (reward or penalty), and updates its strategy.

**Example:**  
AI learning to play a game like AlphaGo.

## Summary

| Learning Type           | Data Type     | Goal                                 | Example                     |
|--------------------------|---------------|--------------------------------------|------------------------------|
| Supervised Learning      | Labeled       | Predict output for new data          | Salary, spam detection       |
| Unsupervised Learning    | Unlabeled     | Find hidden patterns                 | Customer segmentation, PCA   |
| Semi-Supervised Learning | Partly Labeled| Combine both methods                 | Google Photos face grouping  |
| Reinforcement Learning   | Interaction   | Learn via rewards and punishments    | AlphaGo, self-driving cars   |

---

# Batch Learning 

Machine Learning models can also be categorized based on **how they are trained and updated in production**.

## 1. Batch Learning (Offline Learning)

**Definition:**  
The model is trained all at once using the entire dataset.

**Process:**
1. The model is trained offline using all available data.  
2. The trained model is deployed to production.  
3. Once deployed, it becomes static and does not learn further.

**Example:**  
A movie recommendation system trained on all user data up to a certain date.  
It predicts movies based on past knowledge but does not adapt to new trends automatically.

**Key Disadvantage:**  
The model becomes **stale** over time.  
To update it with new data, the entire model must be **retrained from scratch**, which is time- and resource-intensive.

## 3. Comparison and Use Cases

| Aspect                  | Batch Learning                          | Online Learning                          |
|--------------------------|-----------------------------------------|------------------------------------------|
| **Training Style**       | All data at once                        | Incremental (data arrives continuously)  |
| **Adaptability**         | Static; retrain manually                | Dynamic; updates automatically           |
| **Speed of Adaptation**  | Slow; depends on retraining cycles      | Immediate; learns as data changes        |
| **Resource Usage**       | Heavy retraining cost                   | Continuous light updates                 |
| **Use When**             | Data is limited or stable               | Data is streaming or rapidly changing    |
| **Examples**             | Salary prediction, movie recommendation | Stock prediction, fraud detection, news feeds |

**Summary:**  
Choose **Batch Learning** when data changes slowly and retraining occasionally is acceptable.  
Choose **Online Learning** when data arrives continuously and quick adaptation is critical.

---

# Online Learning (Incremental Learning)

## Definition

Online Learning, also called **Incremental Learning**, is a machine learning approach where a model is trained **continuously and incrementally**.  
Unlike traditional **Batch Learning**, where the model is trained once on the entire dataset, Online Learning updates the model **in small steps** — even one data point at a time.

The model is often trained and updated **directly on the production server**.  
With every new data point, the model improves its performance over time.

## How Online Learning Works

1. Start with an initial model trained on some initial data.  
2. Deploy it to production.  
3. Once live, the model:
   - Makes predictions for users.
   - Continuously learns from new incoming data.

Each new data point helps refine and improve the model’s performance.

## Examples

- **Chatbots:** Systems like Google Assistant, Siri, and Alexa improve as users interact with them.  
- **Smartphone Keyboards:** Apps like SwiftKey learn and adapt to your typing patterns.  
- **YouTube Recommendations:** Recommendations update instantly based on what you just watched.

## When to Use Online Learning

Use Online Learning when:

- **The problem changes rapidly** (Concept Drift).  
  _Example:_ Social media trends or changing e-commerce preferences.

- **The dataset is very large.**  
  It’s more efficient to train in small chunks instead of loading the full dataset at once.

- **You need a fast, adaptive system.**  
  The model must respond immediately to new data patterns.

- **System memory is limited.**  
  Example: A 50 GB dataset on a machine with only 8 GB RAM can be processed sequentially in small batches.

## How to Implement Online Learning

### 1. Algorithms that Support Incremental Updates
- **Linear Regression** (with incremental solvers)
- **Stochastic Gradient Descent (SGD)** — naturally supports online updates

### 2. Libraries for Online Learning
- **River:** Python library for online and streaming data.  
- **Vowpal Wabbit:** Scalable and widely used library for incremental learning.

## Challenges and Limitations

### 1. System Complexity and Monitoring
- Building a live, continuously learning system is complex.  
- Requires stable pipelines and real-time data handling.  
- Few reliable enterprise-grade tools exist.

### 2. Risk of Corrupted or Biased Data
- If input data is corrupted (due to hacks or errors), the model can learn wrong patterns.  
- **Solution:**  
  - Use anomaly detection to flag abnormal data.  
  - Maintain backups and restore stable versions when needed.

## Online Learning vs. Batch Learning

| Feature                        | Batch Learning                                 | Online Learning                                  |
|--------------------------------|------------------------------------------------|--------------------------------------------------|
| **Complexity**                 | Simple                                         | More complex to deploy and maintain              |
| **Training Load**              | High (entire dataset at once)                  | Spread out over time                             |
| **Implementation**             | Easier                                         | Harder                                           |
| **Adaptability**               | Static                                         | Continuously adapts to new data                 |
| **Best Suited For**            | Stable, slow-changing data                     | Rapidly changing or streaming data               |
| **Tools Availability**         | Many mature frameworks                         | Fewer dedicated tools                            |

## Summary

Online Learning enables models to **learn on the go**, updating themselves as new data arrives.  
It is ideal for **dynamic, high-volume, or fast-changing environments**, but requires strong **monitoring, data validation, and system design** to avoid corrupted learning.

---

# Instance-Based Learning vs. Model-Based Learning

## Introduction

Machine learning algorithms learn in two main ways:

1. **Instance-Based Learning (Memory-Based Learning):**  
   Learns by **memorizing**. The algorithm stores the training data and compares new data points against stored examples to make predictions.

2. **Model-Based Learning:**  
   Learns by **understanding patterns**. The algorithm finds a mathematical relationship or general rule from the data and uses it for predictions.

## 1. Instance-Based Learning

**Core Idea:**  
The model memorizes the entire training dataset. There is no explicit model-building phase; learning happens at prediction time.

**How It Works (Example):**  
Predicting student placement (Yes/No) using IQ and CGPA:
1. Store all training data points (labeled as "Placed" or "Not Placed").
2. For a new student, compute its distance to all stored points.
3. Select the *k* nearest points.
4. Predict based on the majority class of these neighbors.

**Key Characteristics:**
- No separate training phase (lazy learning).
- Predictions depend on similarity to stored data.
- Entire training data must be available for predictions.

**Example Algorithm:**  
K-Nearest Neighbors (KNN)

## 2. Model-Based Learning

**Core Idea:**  
The model builds a mathematical representation (a rule or function) that captures the relationship between input and output.

**How It Works (Example):**  
Predicting student placement using IQ and CGPA:
1. Analyze data to find a **decision boundary** separating classes.
2. This boundary becomes the **model**.
3. For a new data point, check which side of the boundary it lies on.

**Key Characteristics:**
- Involves a distinct training phase.
- Learns parameters that generalize the relationship.
- Only the model is needed for future predictions; raw data can be discarded.

**Example Algorithms:**  
Linear Regression, Logistic Regression, Decision Trees, Neural Networks.

## 3. Comparison

| Feature | Instance-Based Learning | Model-Based Learning |
|----------|-------------------------|----------------------|
| **Data Preprocessing** | Required equally for both. | Required equally for both. |
| **Model Generation** | No model is built; data itself is used. | Builds an explicit model. |
| **Feedback & Parameters** | No parameters to tune. | Parameters are tuned during training. |
| **Training Phase** | None (lazy learning). | Requires training to build model. |
| **Stored Data** | Full training dataset retained. | Only the final model is stored. |
| **Focus** | Based on similarity between data points. | Based on generalizing relationships. |
| **Generalization** | Happens during prediction. | Happens during training. |
| **New Data Handling** | Refers to all stored data for each new input. | Uses the learned model directly. |
| **Storage Requirement** | High (entire dataset stored). | Low (only model stored). |
| **Prediction Speed** | Slower (distance computation needed). | Fast (direct model evaluation). |
| **Training Speed** | Fast (just stores data). | Can be computationally intensive. |

## Conclusion

- **Instance-Based Learning:** Memorizes examples and predicts using similarity.  
- **Model-Based Learning:** Learns general rules and predicts using the trained model.  
Understanding whether an algorithm is instance-based or model-based helps determine its memory use, training cost, and prediction behavior.

---

# Challenges in Machine Learning

Machine Learning (ML) helps systems learn from data, but several real-world challenges make it difficult to build effective models.

## 1. Data Collection
- Collecting large, relevant, and clean datasets is often hard.
- Real-world data may be incomplete, inconsistent, or unstructured.
- Web scraping and APIs help, but still need heavy preprocessing.

## 2. Insufficient or Poor-Quality Data
- Small or noisy datasets produce unreliable models.
- High-quality, labeled data is costly and time-consuming to obtain.

## 3. Unrepresentative Data
- If data doesn’t represent the full population, predictions become biased.
- Example: Using only Indian survey data to predict global outcomes.

## 4. Sampling Bias
- Even large datasets can be biased if not balanced across all categories.
- Must ensure fair and diverse sampling.

## 5. Data Quality Issues
- Missing values, duplicates, and inconsistent formats degrade performance.
- Data cleaning and preprocessing take up to 60–70% of total project time.

## 6. Irrelevant or Redundant Features
- Unnecessary features add noise and confusion.
- Feature selection and engineering improve accuracy.

## 7. Overfitting
- Model memorizes training data instead of learning patterns.
- Performs well on training data but poorly on unseen data.

## 8. Underfitting
- Model is too simple to capture data complexity.
- Performs poorly on both training and test data.

## 9. Integration with Software Systems
- Deploying trained models into applications, APIs, or servers can be complex.
- Platform compatibility and performance tuning are major concerns.

### Summary
Building ML models isn’t just about algorithms — it’s about handling data effectively, avoiding bias, and ensuring smooth deployment into real systems.
