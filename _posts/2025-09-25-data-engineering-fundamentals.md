---
layout: default
title: "FUNDAMENTALS OF DATA ENGINEERING"
date: 2025-09-25
---

### 1. What is ERD?
  
It is a visual representation of entities and the relationships between them in a database.  An ERD provides a standardized, high-level blueprint for a database. It allows us to:

- Identify all the key pieces of information (entities).
- Define the data we need to store about them (attributes).
- Map how they are related to one another (relationships).
- Communicate this plan clearly and concisely to everyone on the team.

### 2. SQL vs NoSQL vs CSV — when to use which?
 
Choosing a storage format depends on data type, scale, and usage.

- SQL (e.g., MySQL, PostgreSQL): Best for structured data with a fixed schema. Ideal for applications requiring strong consistency and complex queries, like finance or ERP systems. Pros: ACID-compliant, supports JOINs. Cons: Rigid schema, scales vertically (single server).

- NoSQL (e.g., MongoDB, Cassandra, Redis): Suited for semi-structured or unstructured data, high-volume, and real-time workloads. Common in social media, IoT, and analytics. Pros: Flexible schema, fast, horizontally scalable. Cons: Limited consistency, complex joins.

- CSV (flat files): Simple, human-readable, and portable. Good for small datasets, one-time transfers, or quick prototyping. Pros: Easy to read/write, no setup. Cons: Not scalable, lacks relationships or constraints.

### 3. What is the difference between the structured and unstructured data?
- Structured Data is highly organized, like a spreadsheet. It lives in a fixed schema, making it easy to query with languages like SQL.
- Unstructured Data has no predefined format, like a block of text or an image.
- Semi-Structured Data sits in between, using tags or keys to enforce some hierarchy without a rigid schema.
- The choice depends on the data's nature and how you need to access it.
- A data professional often transforms unstructured data into a structured format for analysis.

### 4. Suppose you have data inernally generated by the company, what will be the issue associated with it?
Integration of this data with modern ML pipelines

### 5. What are Data Privacy and Device Identifiers?
Data privacy refers to the right of individuals to control how their personal data is collected, used, and shared. Device identifiers are unique strings of letters and numbers assigned to a digital device. They allow a device to be recognized and tracked on a network. Examples include the International Mobile Equipment Identity (IMEI) for cell phones or an Advertising ID (IDFA on iOS, GAID on Android) used for marketing.

### 6. What are the challenges of data collection?
Data Quality: Incomplete, inaccurate, or inconsistent records.

Data Integration: Combining data from diverse sources and formats.

Scalability: Managing high volume, velocity, and variety of data.

Legal & Ethical: Ensuring compliance, consent, and secure handling.

### 7. What is the need for balancing Fast vs periodic processing?
Balancing fast (real-time) and periodic (batch) data processing means using a hybrid approach to leverage the benefits of both. Fast processing handles data as it arrives to provide immediate insights for time-sensitive tasks like fraud detection. Periodic processing accumulates data over time and processes it in bulk for less time-critical tasks like monthly reports. 

### 8. What are some of the data-formats?
CSV: Use when you want simplicity + portability.
JSON: Use when you need flexible structure + APIs.
XML: Use when dealing with legacy/enterprise systems.
Parquet/ORC: Use when handling big data analytics at scale.

### 9. NumPy vs Pandas.
NumPy: Python library for numerical computing. Offers fast ndarrays, vectorized operations, and linear algebra. Best for raw numerical data and scientific computing. Limitations: no row/column labels, limited heterogeneous data support.

Pandas: Built on NumPy for data manipulation & analysis. Provides Series (1D) and DataFrame (2D) with labeled data, rich I/O (CSV, JSON, SQL, Parquet), and easy cleaning/transforming. Slightly slower and higher memory usage.

When to Use:
NumPy → mathematical models, matrix ops, scientific computing.
Pandas → structured/tabular data, labeled data, missing values, data cleaning.

### 10. What is the difference between row_major and column_major format?
Row-major stores data by row, making it efficient for Online Transaction Processing (OLTP) where you need to retrieve entire records (e.g., a full user profile).

Column-major stores data by column, making it ideal for Online Analytical Processing (OLAP) where you perform operations on a few columns across many records (e.g., calculating an average). This is the format used in big data analytics file formats like Parquet and ORC.

### 11. what is the difference between the text and the binary formats?
Text Formats (UTF-8, CSV, JSON, XML): Human-readable, portable, easy to debug, but larger and slower to parse. Used for config files, logs, simple data exchange, pipelines.

Binary Formats (Parquet, ORC, Protobuf): Machine-readable, compact, fast, optimized for analytics, but not human-readable and require tools to interpret. Used for big data analytics, ML datasets, high-performance systems, media storage.  

### 12. What is JSON?

Structure: Stores data as key-value pairs (objects) and ordered lists (arrays); supports nested structures, making it hierarchical and document-oriented.

Human-Readable: Plain text format; easy for humans and machines to read and parse.

Performance: Slower than binary formats but can be compressed (e.g., Gzip) for network transfer.

Flexibility & Use Cases: Cross-platform, ideal for APIs, configuration files, logs, and web applications requiring structured or semi-structured data.

### 13. What and why CSV?

The key characteristics of CSV are:
**Structure:** Flat rows and columns; values separated by commas. First row often contains headers.  
**Readability:** Human-readable and easy for machines to parse.  
**Performance:** Not optimized for large datasets; slower than binary formats.  
**Compression:** Compresses well with Gzip due to repetitive patterns.  
**Use Cases:** Data exchange, spreadsheet/database import/export, small-to-medium datasets.

Drawbacks:
Flat structure → cannot store nested/hierarchical data.  
Simple but limited: for complex/nested structures, use **JSON** or **XML**.

### 14. What and why parquet?
Parquet is an open-source, **columnar, binary format** for large datasets. It stores data column by column, enabling **column pruning** (read only needed columns) and **efficient compression**. Not human-readable, it is optimized for performance and storage. Commonly used in **data lakes and analytics** with tools like Apache Spark, Hadoop, AWS Athena, and AWS Glue.

### 15. What is avro?
Avro is a **row-based binary format** designed for efficient data exchange and long-term storage. Each record is stored together, and its **schema is saved in JSON**, enabling flexibility and **schema evolution** (adding/removing fields without breaking compatibility). Avro supports **fast serialization/deserialization**, efficient compression, and is ideal for **write-heavy workloads**, real-time data ingestion, **data streaming pipelines** (e.g., Kafka), and inter-service communication.

### 16. How is Avro helping in changing world?
Avro is widely used in **big data and streaming pipelines** due to its **row-based storage, schema support, and fast serialization**:

1. **Data Streaming / Messaging**  
   Platforms like **Apache Kafka** use Avro to send messages between producers and consumers.  
   Each message is a record with a schema, allowing services to read/write data efficiently and handle schema changes without breaking.

2. **Event Logging / Analytics Pipelines**  
   Web applications, IoT devices, or microservices generate logs/events continuously.  
   Avro allows these events to be **stored compactly** and processed quickly by frameworks like **Apache Flink** or **Spark Streaming**.

3. **Inter-Service Communication in Microservices**  
   Avro provides a consistent **schema contract** for different services.  
   Services can evolve independently while still understanding shared data structures.

4. **Data Storage with Schema Evolution**  
   Long-term storage in **data lakes** or **warehouse staging** often requires schema evolution (adding/removing fields).  
   Avro ensures older data remains readable even after schema updates.

### 17. What is ACID wrt to relational model?
ACID Transactions: Relational databases are built to support ACID properties, which guarantee that database transactions are processed reliably.
Atomicity: The entire transaction either completes successfully or is completely rolled back, leaving no changes.
Consistency: The transaction only brings the database from one valid state to another, ensuring data integrity rules are not violated.
Isolation: Concurrent transactions do not affect each other's execution.
Durability: Once a transaction is committed, its changes are permanent.

### 18. What are the strengths and use cases of relational tables?
The relational model is excellent for use cases that require consistent updates, complex reporting, and managing complex relationships between different data types. Because of its ACID properties and strong integrity rules, it's the ideal choice for Online Transaction Processing (OLTP) systems, such as e-commerce platforms, banking applications, and inventory management systems.

### 19. What is Normalising in relational database?
Normalization is a process for organizing data in a relational database to reduce redundancy and improve data integrity.
Goal: The primary goal is to ensure that data is stored logically and efficiently. This is done by splitting a large, single table into multiple smaller, related tables.

How it Works: You define relationships between these tables using keys. A primary key uniquely identifies a row in one table, and a foreign key in another table links to it. This prevents you from having to repeat the same information across multiple records.

Why It Matters: The process prevents anomalies—insertion, update, and deletion issues that can occur when data is not structured correctly. For example, without normalization, if you need to update a customer's address, you might have to change it in multiple places, creating a risk of inconsistencies.

### 20. What is normalising trade-off?
While normalization is great for transactional systems (OLTP) that handle frequent, small updates, it has a trade-off:

Benefit: It ensures data integrity and saves storage space by eliminating redundancy.

Cost: To get a complete record, you often have to JOIN data from multiple tables. These JOIN operations can be computationally expensive and slow down analytical queries that need to scan large amounts of data.

### 21. What is NoSQL and how is it different from a relational database?
NoSQL stands for "not only SQL" and refers to a broad category of non-relational databases. The main difference is their flexible schema and horizontal scalability. A traditional relational database (SQL) uses a rigid, tabular structure with a predefined schema and relies on joins. NoSQL databases, on the other hand, can store unstructured or semi-structured data without a fixed schema, which makes them highly scalable and better suited for big data and modern application

### 22.What are some of the main types of NoSQL databases and what are their use cases?
There are four main types, but two of the most common are:

Document Databases: They store data in flexible, JSON-like documents. This is great for a product catalog or user profiles where each item can have different attributes. MongoDB is a popular example.

Graph Databases: They store data as nodes and edges to represent relationships. This is perfect for social networks, fraud detection, and recommendation engines where the connections between data are as important as the data itself. Neo4j is a well-known example.

### 23. Can you explain the concept of horizontal scalability and why it's important for NoSQL?
Horizontal scalability means adding more commodity servers to a database cluster to handle increased load, rather than upgrading a single, more powerful server (vertical scalability). NoSQL databases are designed to scale horizontally by distributing data and workload across multiple machines. This is more cost-effective and provides greater flexibility to handle massive volumes of data and user traffic, which is a key requirement for modern web and mobile applications.

### 24. When would you choose a NoSQL database over a relational one?
I would choose a NoSQL database in situations where:
I'm dealing with large volumes of unstructured or semi-structured data.
I need to scale out my application quickly to handle a lot of traffic.
The application requires a flexible schema because the data model is likely to change.
I prioritize high performance for a specific type of query (e.g., retrieving a single document or a graph traversal) over complex, multi-table queries.

### 25. What is a major limitation of NoSQL databases?
The biggest limitation is that most NoSQL databases do not natively support ACID transactions in the same way as traditional relational databases. This makes them less suitable for applications that require strict data integrity and consistency for complex transactions, such as financial and banking systems. However, some NoSQL databases are adding transactional capabilities for specific use cases.

### 26. When to use document model?
In a document model, data is stored in flexible, semi-structured documents, typically in a format like JSON. Each document is a single, self-contained record that can have a different structure from other documents in the same collectionThis model is excellent when your data is used as a single unit. It's fast for retrieving a complete record because you don't need to perform joins.

### 27. When to use graph model?
n a graph model, data is stored as a network of nodes (entities) and edges (relationships). A node represents an entity (like a person or a company), and an edge represents the connection between them (like "is friends with" or "works at").

### 28. If you pick a document database for OLTP, how will you run analytics at scale?
You typically wouldn't run large-scale analytics directly on a document database that is handling OLTP workloads. The two types of workloads have conflicting needs. Instead, the common pattern is a hybrid approach:

ETL/Replication: The data from the OLTP document database is regularly extracted or replicated into a separate data warehouse or data lake that is optimized for analytics.

Transformation: During this process, the data is often denormalized and transformed into a flatter structure to improve query performance.

Columnar Storage: The data is then stored in a columnar format like Parquet. This allows analytical queries to only read the specific columns they need, drastically reducing I/O and speeding up large-scale scans. Tools like AWS Athena or Spark can then query this data lake efficiently.





