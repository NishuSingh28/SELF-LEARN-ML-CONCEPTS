---
layout: default
title: "NLP Word2vec"
date: 2025-12-26
categories: [natural-language-processing]
---

## 1. Why Word Embeddings Are Needed

Machine Learning models cannot work directly with raw text.  
They require numerical input.

Early text representation methods such as:

- One-Hot Encoding  
- Bag of Words  
- TF-IDF  

suffer from critical limitations:

- Very high dimensionality  
- Sparse vectors (mostly zeros)  
- No semantic understanding  
- No notion of similarity between words  

For example, **“happy”** and **“joyful”** are completely unrelated in one-hot encoding.

This motivates **word embeddings**.

---

## 2. What Is a Word Embedding?

A **word embedding** is a dense vector of real numbers that represents a word.

Key properties:

- Low dimensional (typically 100–300 dimensions)  
- Dense (mostly non-zero values)  
- Semantically similar words have similar vectors  
- Supports similarity, clustering, and arithmetic  

In short:

> A word embedding converts words into numbers **while preserving meaning**.

---

## 3. Core Intuition Behind Word2Vec

The fundamental observation behind Word2Vec is:

> **Words that appear in similar contexts tend to have similar meanings.**

Word2Vec does not manually assign vectors.  
Instead, it:

1. Creates a prediction task from text  
2. Trains a shallow neural network  
3. Uses learned internal weights as word vectors  

The embeddings are a **by-product of training**, not the explicit output.

---

## 4. Word2Vec Architectures

### 4.1 CBOW (Continuous Bag of Words)

- Input: surrounding context words  
- Output: target word  

Example (window = 1):

```

watch ___ for
→ campus

```

Characteristics:
- Faster training
- Works well for small datasets

---

### 4.2 Skip-gram

- Input: target word  
- Output: surrounding context words  

Example:

```

campus → watch, for

```

Characteristics:
- Better for large datasets
- Better handling of rare words

---

## 5. Training Data Creation (Sliding Window)

A sliding window moves over text to generate training pairs.

Sentence:
```

watch campus for data science

````

Window size = 1:

| Target  | Context           |
|-------|------------------|
| campus | watch, for       |
| for   | campus, data     |
| data  | for, science     |

This converts raw text into a supervised learning problem.

---

## 6. Neural Network Perspective

Internally, Word2Vec is a **shallow neural network**:

- Input layer: one-hot word(s)  
- Hidden layer: embedding layer  
- Output layer: vocabulary distribution  

The **hidden layer weights** become the word embeddings.

---

## 7. Efficiency Techniques

### 7.1 Negative Sampling
- Trains on correct (positive) word pairs
- Randomly samples incorrect (negative) pairs
- Greatly improves efficiency

### 7.2 Hierarchical Softmax
- Uses a tree-based probability structure
- Faster than full softmax for large vocabularies

---

## 8. What Word2Vec Learns

Word2Vec learns **latent semantic features** automatically.

Examples of relationships captured:

- Gender  
- Profession  
- Topic  
- Contextual similarity  

Individual dimensions are not directly interpretable, but **relationships between vectors are meaningful**.

---

## 9. Loading a Pretrained Word2Vec Model (Gensim)

The transcript demonstrates loading a large pretrained Word2Vec model.

```python
from gensim.models import KeyedVectors

kv = KeyedVectors.load_word2vec_format(
    "GoogleNews-vectors-negative300.bin.gz",
    binary=True
)
````

---

## 10. Accessing a Word Vector

```python
vec = kv["king"]
print(vec.shape)   # (300,)
```

Each word is represented as a dense 300-dimensional vector.

---

## 11. Word Similarity and Analogy Queries

### Most similar words

```python
print(kv.most_similar("king", topn=10))
```

### Cosine similarity

```python
print(kv.similarity("man", "woman"))
```

### Analogy: king − man + woman ≈ queen

```python
print(
    kv.most_similar(
        positive=["king", "woman"],
        negative=["man"],
        topn=5
    )
)
```

---

## 12. Vector Arithmetic

Word vectors support arithmetic operations.

```python
new_vec = kv["king"] - kv["man"] + kv["woman"]
print(kv.similar_by_vector(new_vec, topn=5))
```

This demonstrates how semantic relationships are encoded linearly.

---

## 13. Building a Corpus from Text Files

The transcript shows reading multiple text files, sentence splitting, and tokenization.

```python
from pathlib import Path
from nltk.tokenize import sent_tokenize, word_tokenize

def tokenize_text_file(path):
    text = Path(path).read_text(encoding="utf-8", errors="ignore")
    sentences = sent_tokenize(text)
    return [word_tokenize(s) for s in sentences if len(s) > 0]

corpus = []
for p in Path("data").glob("*.txt"):
    corpus.extend(tokenize_text_file(p))
```

The corpus becomes a list of tokenized sentences.

---

## 14. Training a Word2Vec Model (Gensim)

```python
from gensim.models import Word2Vec

model = Word2Vec(
    vector_size=300,
    window=1,
    min_count=2,
    sg=1,          # Skip-gram
    negative=10,
    workers=4
)

model.build_vocab(corpus)
model.train(
    corpus,
    total_examples=model.corpus_count,
    epochs=5
)
```

---

## 15. Accessing Learned Word Vectors

```python
vec_watch = model.wv["watch"]
print(vec_watch)
```

---

## 16. Saving and Loading Models

```python
# Save full model
model.save("word2vec_full.model")

# Save only word vectors
model.wv.save("word_vectors.kv")

# Load again
# model = Word2Vec.load("word2vec_full.model")
# kv = KeyedVectors.load("word_vectors.kv")
```

---

## 17. Inspecting Vocabulary

```python
print(len(model.wv.index_to_key))
print(model.wv.index_to_key[:50])
```

---

## 18. Visualizing Word Embeddings (PCA)

The transcript demonstrates reducing dimensionality for visualization.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

words = model.wv.index_to_key[:100]
vectors = np.array([model.wv[w] for w in words])

pca = PCA(n_components=2)
coords = pca.fit_transform(vectors)

plt.scatter(coords[:, 0], coords[:, 1], s=10)
for i, w in enumerate(words):
    plt.annotate(w, (coords[i, 0], coords[i, 1]), fontsize=8)
plt.show()
```

---

## 19. Key Advantages of Word2Vec

* Captures semantic meaning
* Dense, low-dimensional vectors
* Efficient computation
* Enables analogy reasoning
* Reduces overfitting compared to sparse methods

---

## 20. Limitations

* Bias from training data
* Cannot handle unseen words
* No explicit sentence-level meaning
* Feature dimensions not interpretable

---

## 21. Conceptual Summary

* Word2Vec learns word meaning from context
* CBOW and Skip-gram are two training strategies
* Embeddings encode similarity in vector space
* Vector arithmetic reveals semantic relationships
* Word2Vec is foundational to modern NLP

---

## 22. Final Insight

> Word2Vec does not understand language.
> It **learns statistical patterns of context so effectively that meaning emerges naturally**.

This idea forms the backbone of modern representation learning in NLP.


